{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "67082072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import argparse\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import scipy.stats as ss\n",
    "from tqdm import tqdm_notebook as tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0ffd568d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'decoding'\n",
      "/Users/gilad/Desktop/Projects/semantic-decoding/decoding\n"
     ]
    }
   ],
   "source": [
    "%cd decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "639f6161",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_DIR = os.getcwd()\n",
    "DATA_LM_DIR = os.path.join(REPO_DIR, \"data_lm\")\n",
    "DATA_TRAIN_DIR = os.path.join(REPO_DIR, \"data_train\")\n",
    "DATA_TEST_DIR = os.path.join(REPO_DIR, \"data_test\")\n",
    "MODEL_DIR = os.path.join(REPO_DIR, \"models\")\n",
    "RESULT_DIR = os.path.join(REPO_DIR, \"results\")\n",
    "SCORE_DIR = os.path.join(REPO_DIR, \"scores\")\n",
    "\n",
    "# GPT encoding model parameters\n",
    "\n",
    "TRIM = 5\n",
    "STIM_DELAYS = [1, 2, 3, 4]\n",
    "RESP_DELAYS = [-4, -3, -2, -1]\n",
    "ALPHAS = np.logspace(1, 3, 10)\n",
    "NBOOTS = 50\n",
    "VOXELS = 10000\n",
    "CHUNKLEN = 40\n",
    "GPT_LAYER = 9\n",
    "GPT_WORDS = 5\n",
    "\n",
    "# decoder parameters\n",
    "\n",
    "RANKED = True\n",
    "WIDTH = 200\n",
    "NM_ALPHA = 2/3\n",
    "LM_TIME = 8\n",
    "LM_MASS = 0.9\n",
    "LM_RATIO = 0.1\n",
    "EXTENSIONS = 5\n",
    "\n",
    "# evaluation parameters\n",
    "\n",
    "WINDOW = 20\n",
    "\n",
    "# devices\n",
    "\n",
    "GPT_DEVICE = \"cpu\"\n",
    "EM_DEVICE = \"cpu\"\n",
    "SM_DEVICE = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f4562975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import config\n",
    "from GPT import GPT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bd25a4",
   "metadata": {},
   "source": [
    "## Decoding Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "41a53a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = \"UTS01\"\n",
    "experiment = \"perceived_speech\"\n",
    "task = \"ifthishaircouldtalk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "929af84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_DIR = os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d01a5007",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LM_DIR = os.path.join(REPO_DIR, \"data_lm\")\n",
    "DATA_TRAIN_DIR = os.path.join(REPO_DIR, \"data_train\")\n",
    "DATA_TEST_DIR = os.path.join(REPO_DIR, \"data_test\")\n",
    "MODEL_DIR = os.path.join(REPO_DIR, \"models\")\n",
    "RESULT_DIR = os.path.join(REPO_DIR, \"results\")\n",
    "SCORE_DIR = os.path.join(REPO_DIR, \"scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "481c9725",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_checkpoint = \"perceived\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "31ffd1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_rate_voxels = \"auditory\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc3cf23",
   "metadata": {},
   "source": [
    "## Load fMRI response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8b6ca940",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h5py.File(os.path.join(DATA_TEST_DIR, \"test_response\", subject, experiment, task + \".hf5\"), \"r\")\n",
    "resp = np.nan_to_num(hf[\"data\"][:])\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "26a4dae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(249, 81126)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c800dbce",
   "metadata": {},
   "source": [
    "## Load GPT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6b697258",
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT = ['i', 'we', 'she', 'he', 'they', 'it']\n",
    "STOPWORDS = {'is', 'does', 's', 'having', 'doing', 'these', 'shan', 'yourself', 'other', 'are', 'hasn', 'at', 'for', 'while', 'down', \"hadn't\", 'until', 'above', 'during', 'each', 'now', 'have', \"won't\", 'once', 'why', 'here', 'ourselves', 'to', 'over', 'into', 'who', 'that', 'myself', 'he', 'themselves', 'were', 'against', 'about', 'some', 'has', 'but', 'ma', 'their', 'this', 'there', 'with', \"that'll\", \"shan't\", \"wouldn't\", 'a', 'those', \"you'll\", 'll', 'few', 'couldn', 'an', 'd', \"weren't\", 'doesn', 'own', 'won', 'didn', 'what', 'when', 'in', 'below', 'where', \"it's\", 'most', 'just', \"you're\", 'yourselves', 'too', \"don't\", \"she's\", \"didn't\", \"hasn't\", 'isn', \"mustn't\", 'of', 'did', 'how', 'himself', 'aren', 'if', 'very', 'or', 'weren', 'it', 'be', 'itself', \"doesn't\", 'my', 'o', 'no', \"isn't\", 'before', 'after', 'off', 'was', 'can', 'the', 'been', 'her', 'him', \"wasn't\", 've', 'through', \"needn't\", 'because', 'nor', 'will', 'm', 't', 'out', 'on', 'she', 'all', 'then', 'than', \"mightn't\", 'hers', 'herself', 'only', 'should', 're', 'ain', 'wasn', \"aren't\", \"couldn't\", 'they', 'hadn', 'had', 'more', 'and', 'under', \"shouldn't\", 'any', 'y', 'don', 'from', 'so', 'whom', 'as', 'mustn', 'between', 'up', 'do', 'both', 'such', 'our', 'its', 'which', 'not', \"haven't\", 'needn', 'by', \"should've\", 'again', 'shouldn', 'his', 'me', 'further', 'yours', 'am', 'your', 'haven', 'wouldn', 'being', 'ours', 'you', 'i', 'theirs', 'mightn', 'same', 'we', \"you've\", 'them', \"you'd\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5473eec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_LAYER = 9\n",
    "GPT_WORDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "937da655",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_DEVICE = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2630ac3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/gilad/Desktop/Projects/semantic-decoding/data_lm/decoder_vocab.json'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(DATA_LM_DIR, \"decoder_vocab.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "71d29bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/gilad/Desktop/Projects/semantic-decoding/data_lm/perceived/vocab.json'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(DATA_LM_DIR, gpt_checkpoint, \"vocab.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0d4398f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_LM_DIR, gpt_checkpoint, \"vocab.json\"), \"r\") as f:\n",
    "    gpt_vocab = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4d9864bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_LM_DIR, \"decoder_vocab.json\"), \"r\") as f:\n",
    "    decoder_vocab = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "32a2e478",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "50ba6add",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMFeatures():\n",
    "    \"\"\"class for extracting contextualized features of stimulus words\n",
    "    \"\"\"\n",
    "    def __init__(self, model, layer, context_words):\n",
    "        self.model, self.layer, self.context_words = model, layer, context_words\n",
    "\n",
    "    def extend(self, extensions, verbose = False):\n",
    "        \"\"\"outputs array of vectors corresponding to the last words of each extension\n",
    "        \"\"\"\n",
    "        contexts = [extension[-(self.context_words+1):] for extension in extensions]\n",
    "        if verbose: print(contexts)\n",
    "        context_array = self.model.get_context_array(contexts)\n",
    "        embs = self.model.get_hidden(context_array, layer = self.layer)\n",
    "        return embs[:, len(contexts[0]) - 1]\n",
    "\n",
    "    def make_stim(self, words):\n",
    "        \"\"\"outputs matrix of features corresponding to the stimulus words\n",
    "        \"\"\"\n",
    "        context_array = self.model.get_story_array(words, self.context_words)\n",
    "        embs = self.model.get_hidden(context_array, layer = self.layer)\n",
    "        return np.vstack([embs[0, :self.context_words], \n",
    "            embs[:context_array.shape[0] - self.context_words, self.context_words]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ef418d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nucleus(probs, nuc_mass, nuc_ratio):\n",
    "    \"\"\"identify words that constitute a given fraction of the probability mass\n",
    "    \"\"\"\n",
    "    nuc_ids = np.where(probs >= np.max(probs) * nuc_ratio)[0]\n",
    "    nuc_pairs = sorted(zip(nuc_ids, probs[nuc_ids]), key = lambda x : -x[1]) \n",
    "    sum_mass = np.cumsum([x[1] for x in nuc_pairs])\n",
    "    cutoffs = np.where(sum_mass >= nuc_mass)[0]\n",
    "    if len(cutoffs) > 0: nuc_pairs = nuc_pairs[:cutoffs[0]+1]\n",
    "    nuc_ids = [x[0] for x in nuc_pairs]                     \n",
    "    return nuc_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4c047a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_filter(proposals, context):\n",
    "    \"\"\"filter out words that occur in a context to prevent repetitions\n",
    "    \"\"\"\n",
    "    cut_words = []\n",
    "    cut_words.extend([context[i+1] for i, word in enumerate(context[:-1]) if word == context[-1]]) # bigrams\n",
    "    cut_words.extend([x for x in proposals if x not in STOPWORDS and in_context(x, context)]) # unigrams\n",
    "    return [x for x in proposals if x not in cut_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "df36430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_context(word, context):\n",
    "    \"\"\"test whether [word] or a stem of [word] is in [context]\n",
    "    \"\"\"\n",
    "    stem_context = [stemmer.stem(x) for x in context]\n",
    "    stem_word = stemmer.stem(word)\n",
    "    return (stem_word in stem_context or stem_word in context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1eaea77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel():\n",
    "    \"\"\"class for generating word sequences using a language model\n",
    "    \"\"\"\n",
    "    def __init__(self, model, vocab, nuc_mass = 1.0, nuc_ratio = 0.0):        \n",
    "        self.model = model\n",
    "        self.ids = {i for word, i in self.model.word2id.items() if word in set(vocab)}\n",
    "        self.nuc_mass, self.nuc_ratio = nuc_mass, nuc_ratio\n",
    "        \n",
    "    def ps(self, contexts):\n",
    "        \"\"\"get probability distributions over the next words for each context\n",
    "        \"\"\"\n",
    "        context_arr = self.model.get_context_array(contexts)\n",
    "        probs = self.model.get_probs(context_arr)\n",
    "        return probs[:, len(contexts[0]) - 1] \n",
    "    \n",
    "    def beam_propose(self, beam, context_words):\n",
    "        \"\"\"get possible extension words for each hypothesis in the decoder beam\n",
    "        \"\"\"\n",
    "        if len(beam) == 1: \n",
    "            nuc_words = [w for w in INIT if self.model.word2id[w] in self.ids]\n",
    "            nuc_logprobs = np.log(np.ones(len(nuc_words)) / len(nuc_words))\n",
    "            return [(nuc_words, nuc_logprobs)]\n",
    "        else:\n",
    "            contexts = [hyp.words[-context_words:] for hyp in beam]\n",
    "            beam_probs = self.ps(contexts)\n",
    "            beam_nucs = []\n",
    "            for context, probs in zip(contexts, beam_probs):\n",
    "                nuc_ids = get_nucleus(probs, nuc_mass = self.nuc_mass, nuc_ratio = self.nuc_ratio)\n",
    "                nuc_words = [self.model.vocab[i] for i in nuc_ids if i in self.ids]\n",
    "                nuc_words = context_filter(nuc_words, context)\n",
    "                nuc_logprobs = np.log([probs[self.model.word2id[w]] for w in nuc_words])\n",
    "                beam_nucs.append((nuc_words, nuc_logprobs))\n",
    "            return beam_nucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e76abe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPT(path = os.path.join(DATA_LM_DIR, gpt_checkpoint, \"model\"), vocab = gpt_vocab, device = GPT_DEVICE)\n",
    "features = LMFeatures(model = gpt, layer = GPT_LAYER, context_words = GPT_WORDS)\n",
    "lm = LanguageModel(gpt, decoder_vocab, nuc_mass = LM_MASS, nuc_ratio = LM_RATIO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4e9b5c",
   "metadata": {},
   "source": [
    "## Load Encoding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f2daffdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/gilad/Desktop/Projects/semantic-decoding/decoding'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9c397449",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = \"/Users/gilad/Desktop/Projects/semantic-decoding/decoding/models/UTS01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6b23d1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"encoding_model_percieved.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5a09b727",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_location = os.path.join(MODEL_DIR, subject)\n",
    "word_rate_model = np.load(os.path.join(load_location, \"word_rate_model_%s.npz\" % word_rate_voxels), allow_pickle = True)\n",
    "#encoding_model = np.load(os.path.join(load_location, \"encoding_model_%s.npz\" % gpt_checkpoint))\n",
    "encoding_model = np.load(models_dir + \"/\" + model_file)\n",
    "weights = encoding_model[\"weights\"]\n",
    "noise_model = encoding_model[\"noise_model\"]\n",
    "tr_stats = encoding_model[\"tr_stats\"]\n",
    "word_stats = encoding_model[\"word_stats\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e55453e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_delayed(stim, delays, circpad=False):\n",
    "    \"\"\"Creates non-interpolated concatenated delayed versions of [stim] with the given [delays] \n",
    "    (in samples).\n",
    "    \n",
    "    If [circpad], instead of being padded with zeros, [stim] will be circularly shifted.\n",
    "    \"\"\"\n",
    "    nt,ndim = stim.shape\n",
    "    dstims = []\n",
    "    for di,d in enumerate(delays):\n",
    "        dstim = np.zeros((nt, ndim))\n",
    "        if d<0: ## negative delay\n",
    "            dstim[:d,:] = stim[-d:,:]\n",
    "            if circpad:\n",
    "                dstim[d:,:] = stim[:-d,:]\n",
    "        elif d>0:\n",
    "            dstim[d:,:] = stim[:-d,:]\n",
    "            if circpad:\n",
    "                dstim[:d,:] = stim[-d:,:]\n",
    "        else: ## d==0\n",
    "            dstim = stim.copy()\n",
    "        dstims.append(dstim)\n",
    "    return np.hstack(dstims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2992da3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_word_rate(resp, wt, vox, mean_rate):\n",
    "    \"\"\"predict word rate at each acquisition time\n",
    "    \"\"\"\n",
    "    delresp = make_delayed(resp[:, vox], RESP_DELAYS)\n",
    "    rate = ((delresp.dot(wt) + mean_rate)).reshape(-1).clip(min = 0)\n",
    "    return np.round(rate).astype(int)\n",
    "\n",
    "def predict_word_times(word_rate, resp, starttime = 0, tr = 2):\n",
    "    \"\"\"predict evenly spaced word times from word rate\n",
    "    \"\"\"\n",
    "    half = tr / 2\n",
    "    trf = TRFile(None, tr)\n",
    "    trf.soundstarttime = starttime\n",
    "    trf.simulate(resp.shape[0])\n",
    "    tr_times = trf.get_reltriggertimes() + half\n",
    "\n",
    "    word_times = []\n",
    "    for mid, num in zip(tr_times, word_rate):  \n",
    "        if num < 1: continue\n",
    "        word_times.extend(np.linspace(mid - half, mid + half, num, endpoint = False) + half / num)\n",
    "    return np.array(word_times), tr_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "32ed1da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EM_DEVICE = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "02adcedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodingModel():\n",
    "    \"\"\"class for computing the likelihood of observing brain recordings given a word sequence\n",
    "    \"\"\"\n",
    "    def __init__(self, resp, weights, voxels, sigma, device = \"cpu\"):\n",
    "        self.device = device\n",
    "        self.weights = torch.from_numpy(weights[:, voxels]).float().to(self.device)\n",
    "        self.resp = torch.from_numpy(resp[:, voxels]).float().to(self.device)\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def set_shrinkage(self, alpha):\n",
    "        \"\"\"compute precision from empirical covariance with shrinkage factor alpha\n",
    "        \"\"\"\n",
    "        precision = np.linalg.inv(self.sigma * (1 - alpha) + np.eye(len(self.sigma)) * alpha)\n",
    "        self.precision = torch.from_numpy(precision).float().to(self.device)\n",
    "\n",
    "    def prs(self, stim, trs):\n",
    "        \"\"\"compute P(R | S) on affected TRs for each hypothesis\n",
    "        \"\"\"\n",
    "        with torch.no_grad(): \n",
    "            stim = stim.float().to(self.device)\n",
    "            diff = torch.matmul(stim, self.weights) - self.resp[trs] # encoding model residuals\n",
    "            multi = torch.matmul(torch.matmul(diff, self.precision), diff.permute(0, 2, 1))\n",
    "            return -0.5 * multi.diagonal(dim1 = -2, dim2 = -1).sum(dim = 1).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "97b506ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "em = EncodingModel(resp, weights, encoding_model[\"voxels\"], noise_model, device = EM_DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ac064816",
   "metadata": {},
   "outputs": [],
   "source": [
    "NM_ALPHA =  2/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "38a33fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "em.set_shrinkage(NM_ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "02818283",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRFile(object):\n",
    "    def __init__(self, trfilename, expectedtr=2.0045):\n",
    "        \"\"\"Loads data from [trfilename], should be output from stimulus presentation code.\n",
    "        \"\"\"\n",
    "        self.trtimes = []\n",
    "        self.soundstarttime = -1\n",
    "        self.soundstoptime = -1\n",
    "        self.otherlabels = []\n",
    "        self.expectedtr = expectedtr\n",
    "        \n",
    "        if trfilename is not None:\n",
    "            self.load_from_file(trfilename)\n",
    "        \n",
    "\n",
    "    def load_from_file(self, trfilename):\n",
    "        \"\"\"Loads TR data from report with given [trfilename].\n",
    "        \"\"\"\n",
    "        ## Read the report file and populate the datastructure\n",
    "        for ll in open(trfilename):\n",
    "            timestr = ll.split()[0]\n",
    "            label = \" \".join(ll.split()[1:])\n",
    "            time = float(timestr)\n",
    "\n",
    "            if label in (\"init-trigger\", \"trigger\"):\n",
    "                self.trtimes.append(time)\n",
    "\n",
    "            elif label==\"sound-start\":\n",
    "                self.soundstarttime = time\n",
    "\n",
    "            elif label==\"sound-stop\":\n",
    "                self.soundstoptime = time\n",
    "\n",
    "            else:\n",
    "                self.otherlabels.append((time, label))\n",
    "        \n",
    "        ## Fix weird TR times\n",
    "        itrtimes = np.diff(self.trtimes)\n",
    "        badtrtimes = np.nonzero(itrtimes>(itrtimes.mean()*1.5))[0]\n",
    "        newtrs = []\n",
    "        for btr in badtrtimes:\n",
    "            ## Insert new TR where it was missing..\n",
    "            newtrtime = self.trtimes[btr]+self.expectedtr\n",
    "            newtrs.append((newtrtime,btr))\n",
    "\n",
    "        for ntr,btr in newtrs:\n",
    "            self.trtimes.insert(btr+1, ntr)\n",
    "\n",
    "    def simulate(self, ntrs):\n",
    "        \"\"\"Simulates [ntrs] TRs that occur at the expected TR.\n",
    "        \"\"\"\n",
    "        self.trtimes = list(np.arange(ntrs)*self.expectedtr)\n",
    "    \n",
    "    def get_reltriggertimes(self):\n",
    "        \"\"\"Returns the times of all trigger events relative to the sound.\n",
    "        \"\"\"\n",
    "        return np.array(self.trtimes)-self.soundstarttime\n",
    "\n",
    "    @property\n",
    "    def avgtr(self):\n",
    "        \"\"\"Returns the average TR for this run.\n",
    "        \"\"\"\n",
    "        return np.diff(self.trtimes).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "48ee03f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lanczosfun(cutoff, t, window=3):\n",
    "    \"\"\"Compute the lanczos function with some cutoff frequency [B] at some time [t].\n",
    "    [t] can be a scalar or any shaped numpy array.\n",
    "    If given a [window], only the lowest-order [window] lobes of the sinc function\n",
    "    will be non-zero.\n",
    "    \"\"\"\n",
    "    t = t * cutoff\n",
    "    val = window * np.sin(np.pi*t) * np.sin(np.pi*t/window) / (np.pi**2 * t**2)\n",
    "    val[t==0] = 1.0\n",
    "    val[np.abs(t)>window] = 0.0\n",
    "    return val# / (val.sum() + 1e-10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "dd797e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lanczos_mat(oldtime, newtime, window = 3, cutoff_mult = 1.0, rectify = False):\n",
    "    \"\"\"get matrix for downsampling from TR times to word times\n",
    "    \"\"\"\n",
    "    cutoff = 1 / np.mean(np.diff(newtime)) * cutoff_mult\n",
    "    sincmat = np.zeros((len(newtime), len(oldtime)))\n",
    "    for ndi in range(len(newtime)):\n",
    "        sincmat[ndi,:] = lanczosfun(cutoff, newtime[ndi] - oldtime, window)\n",
    "    return sincmat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d040fd7e",
   "metadata": {},
   "source": [
    "## predict word times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9875e22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_rate = predict_word_rate(resp, word_rate_model[\"weights\"], word_rate_model[\"voxels\"], word_rate_model[\"mean_rate\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1337b933",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_times, tr_times = predict_word_times(word_rate, resp, starttime = -10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "610acbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mr/9jcsmzqd1312jvpkw6kf0y0c0000gn/T/ipykernel_82114/4071827947.py:8: RuntimeWarning: invalid value encountered in divide\n",
      "  val = window * np.sin(np.pi*t) * np.sin(np.pi*t/window) / (np.pi**2 * t**2)\n"
     ]
    }
   ],
   "source": [
    "lanczos_mat = get_lanczos_mat(word_times, tr_times)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76cc7f3",
   "metadata": {},
   "source": [
    "## decode responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "382d43fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hypothesis(object):\n",
    "    \"\"\"a class for representing word sequence hypotheses\n",
    "    \"\"\"\n",
    "    def __init__(self, parent = None, extension = None):\n",
    "        if parent is None: \n",
    "            self.words, self.logprobs, self.embs = [], [], []\n",
    "        else:\n",
    "            word, logprob, emb = extension\n",
    "            self.words = parent.words + [word]\n",
    "            self.logprobs = parent.logprobs + [logprob]\n",
    "            self.embs = parent.embs + [emb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "afbd832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(object):\n",
    "    \"\"\"class for beam search decoding\n",
    "    \"\"\"\n",
    "    def __init__(self, word_times, beam_width, extensions = 5):\n",
    "        self.word_times = word_times\n",
    "        self.beam_width, self.extensions = beam_width, extensions\n",
    "        self.beam = [Hypothesis()] # initialize with empty hypothesis\n",
    "        self.scored_extensions = [] # global extension pool\n",
    "        \n",
    "    def first_difference(self):\n",
    "        \"\"\"get first index where hypotheses on the beam differ\n",
    "        \"\"\"\n",
    "        words_arr = np.array([hypothesis.words for hypothesis in self.beam])\n",
    "        if words_arr.shape[0] == 1: return words_arr.shape[1]\n",
    "        for index in range(words_arr.shape[1]): \n",
    "            if len(set(words_arr[:, index])) > 1: return index\n",
    "        return 0\n",
    "    \n",
    "    def time_window(self, sample_index, seconds, floor = 0):\n",
    "        \"\"\"number of prior words within [seconds] of the currently sampled time point\"\"\"\n",
    "        window = [time for time in self.word_times if time < self.word_times[sample_index] \n",
    "                  and time > self.word_times[sample_index] - seconds]\n",
    "        return max(len(window), floor)\n",
    "        \n",
    "    def get_hypotheses(self):\n",
    "        \"\"\"get the number of permitted extensions for each hypothesis on the beam\n",
    "        \"\"\"\n",
    "#        print(\"get hypotheses\")\n",
    "        if len(self.beam[0].words) == 0: \n",
    "            return zip(self.beam, [self.extensions for hypothesis in self.beam])\n",
    "        logprobs = [sum(hypothesis.logprobs) for hypothesis in self.beam]\n",
    "        num_extensions = [int(np.ceil(self.extensions * rank / len(logprobs))) for \n",
    "                          rank in ss.rankdata(logprobs)]\n",
    "        \n",
    "        return zip(self.beam, num_extensions)\n",
    "    \n",
    "    def add_extensions(self, extensions, likelihoods, num_extensions):\n",
    "        \"\"\"add extensions for each hypothesis to global extension pool\n",
    "        \"\"\"\n",
    "        scored_extensions = sorted(zip(extensions, likelihoods), key = lambda x : -x[1])\n",
    "        self.scored_extensions.extend(scored_extensions[:num_extensions])\n",
    "\n",
    "    def extend(self, verbose = False):\n",
    "        \"\"\"update beam based on global extension pool \n",
    "        \"\"\"\n",
    "        self.beam = [x[0] for x in sorted(self.scored_extensions, key = lambda x : -x[1])[:self.beam_width]]\n",
    "        self.scored_extensions = []\n",
    "        if verbose: print(self.beam[0].words)\n",
    "        \n",
    "    def save(self, path):\n",
    "        \"\"\"save decoder results\n",
    "        \"\"\"\n",
    "        np.savez(path, words = np.array(self.beam[0].words), times = np.array(self.word_times))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3a855f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def affected_trs(start_index, end_index, lanczos_mat, delay = True):\n",
    "    \"\"\"identify TRs influenced by words in the range [start_index, end_index]\n",
    "    \"\"\"\n",
    "    start_tr, end_tr = np.where(lanczos_mat[:, start_index])[0][0], np.where(lanczos_mat[:, end_index])[0][-1]\n",
    "    start_tr, end_tr = start_tr + min(STIM_DELAYS), end_tr + max(STIM_DELAYS)\n",
    "    start_tr, end_tr = max(start_tr, 0), min(end_tr, lanczos_mat.shape[0] - 1)\n",
    "    return np.arange(start_tr, end_tr + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "dc52025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StimulusModel():\n",
    "    \"\"\"class for constructing stimulus features\n",
    "    \"\"\"\n",
    "    def __init__(self, lanczos_mat, tr_stats, word_mean, device = 'cpu'):\n",
    "        self.device = device\n",
    "        self.lanczos_mat = torch.from_numpy(lanczos_mat).float().to(self.device)\n",
    "        self.tr_mean = torch.from_numpy(tr_stats[0]).float().to(device)\n",
    "        self.tr_std_inv = torch.from_numpy(np.diag(1 / tr_stats[1])).float().to(device)\n",
    "        self.blank = torch.from_numpy(word_mean).float().to(self.device)\n",
    "        \n",
    "    def _downsample(self, variants):\n",
    "        \"\"\"downsamples word embeddings to TR embeddings for each hypothesis\n",
    "        \"\"\"\n",
    "        return torch.matmul(self.lanczos_mat.unsqueeze(0), variants)\n",
    "    \n",
    "    def _normalize(self, tr_variants):\n",
    "        \"\"\"normalize TR embeddings for each hypothesis\n",
    "        \"\"\"\n",
    "        centered = tr_variants - self.tr_mean\n",
    "        return torch.matmul(centered, self.tr_std_inv)\n",
    "\n",
    "    def _delay(self, tr_variants, n_vars, n_feats):\n",
    "        \"\"\"apply finite impulse response delays to TR embeddings\n",
    "        \"\"\"\n",
    "        delays = STIM_DELAYS\n",
    "        n_trs = tr_variants.shape[1]\n",
    "        del_tr_variants = torch.zeros(n_vars, n_trs, len(delays)*n_feats).to(self.device)\n",
    "        for c, d in enumerate(delays): \n",
    "            feat_ind_start = c * n_feats\n",
    "            feat_ind_end = (c + 1) * n_feats\n",
    "            del_tr_variants[:, d:, feat_ind_start:feat_ind_end] = tr_variants[:, :n_trs - d, :]\n",
    "        return del_tr_variants\n",
    "        \n",
    "    def make_variants(self, sample_index, hypothesis_embs, var_embs, affected_trs):\n",
    "        \"\"\"create stimulus features for each hypothesis\n",
    "        \"\"\"\n",
    "        n_variants, n_feats = len(var_embs), self.blank.shape[0]\n",
    "        with torch.no_grad():\n",
    "            full = self.blank.repeat(self.lanczos_mat.shape[1], 1) # word times x features\n",
    "            full[:sample_index] = torch.tensor(np.array(hypothesis_embs)).float().reshape(-1, n_feats).to(self.device)\n",
    "            variants = full.repeat(n_variants, 1, 1) # variants x word times x features\n",
    "            variants[:, sample_index, :] = torch.tensor(np.array(var_embs)).float().to(self.device)\n",
    "            tr_variants = self._normalize(self._downsample(variants))\n",
    "            del_tr_variants = self._delay(tr_variants, n_variants, n_feats)\n",
    "        return del_tr_variants[:, affected_trs, :].to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f9ea5b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(word_times, WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "087de733",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = StimulusModel(lanczos_mat, tr_stats, word_stats[0], device = SM_DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ef7daa17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0x7f9b42a72280>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.get_hypotheses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "98f84683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mr/9jcsmzqd1312jvpkw6kf0y0c0000gn/T/ipykernel_82114/989296795.py:1: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for sample_index in tqdm(range(len(word_times))):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1136bd306bd54975b2faef235f832785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1360 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[137], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(nuc) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     12\u001b[0m         extend_words \u001b[38;5;241m=\u001b[39m [hyp\u001b[38;5;241m.\u001b[39mwords \u001b[38;5;241m+\u001b[39m [x] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m nuc]\n\u001b[0;32m---> 13\u001b[0m         extend_embs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextend_words\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#         print(\"sample_index\", sample_index)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#         print(\"len(hyp.embs)\", len(hyp.embs))\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#         print(\"len(extend_embs)\", len(extend_embs) )\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#         print(\"trs\", trs)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m         stim \u001b[38;5;241m=\u001b[39m sm\u001b[38;5;241m.\u001b[39mmake_variants(sample_index, hyp\u001b[38;5;241m.\u001b[39membs, extend_embs, trs)\n",
      "Cell \u001b[0;32mIn[102], line 13\u001b[0m, in \u001b[0;36mLMFeatures.extend\u001b[0;34m(self, extensions, verbose)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose: \u001b[38;5;28mprint\u001b[39m(contexts)\n\u001b[1;32m     12\u001b[0m context_array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_context_array(contexts)\n\u001b[0;32m---> 13\u001b[0m embs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_hidden\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embs[:, \u001b[38;5;28mlen\u001b[39m(contexts[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/Projects/semantic-decoding/decoding/GPT.py:43\u001b[0m, in \u001b[0;36mGPT.get_hidden\u001b[0;34m(self, ids, layer)\u001b[0m\n\u001b[1;32m     41\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(ids\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;241m.\u001b[39mint()\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 43\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mhidden_states[layer]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/semantic_decoding/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/semantic_decoding/lib/python3.9/site-packages/transformers/models/openai/modeling_openai.py:575\u001b[0m, in \u001b[0;36mOpenAIGPTLMHeadModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    573\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 575\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    586\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    587\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/semantic_decoding/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/semantic_decoding/lib/python3.9/site-packages/transformers/models/openai/modeling_openai.py:504\u001b[0m, in \u001b[0;36mOpenAIGPTModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    502\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[0;32m--> 504\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/semantic_decoding/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/semantic_decoding/lib/python3.9/site-packages/transformers/models/openai/modeling_openai.py:252\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, head_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 252\u001b[0m     attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m     a \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    260\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(x \u001b[38;5;241m+\u001b[39m a)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/semantic_decoding/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/semantic_decoding/lib/python3.9/site-packages/transformers/models/openai/modeling_openai.py:210\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, head_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 210\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m     query, key, value \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_size, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    212\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_heads(query)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/semantic_decoding/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/semantic_decoding/lib/python3.9/site-packages/transformers/pytorch_utils.py:103\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    102\u001b[0m     size_out \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnf,)\n\u001b[0;32m--> 103\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddmm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(size_out)\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for sample_index in tqdm(range(len(word_times))):\n",
    "    trs = affected_trs(decoder.first_difference(), sample_index, lanczos_mat)\n",
    "    ncontext = decoder.time_window(sample_index, LM_TIME, floor = 5)\n",
    "    beam_nucs = lm.beam_propose(decoder.beam, ncontext)\n",
    "#    print(beam_nucs)\n",
    "    for c, (hyp, nextensions) in enumerate(decoder.get_hypotheses()):\n",
    "#        print(c)\n",
    "        nuc, logprobs = beam_nucs[c]\n",
    "#        print(nuc)\n",
    "#        print(\"hyp.embs: \", hyp.embs)\n",
    "        if len(nuc) < 1: continue\n",
    "        extend_words = [hyp.words + [x] for x in nuc]\n",
    "        extend_embs = list(features.extend(extend_words))\n",
    "#         print(\"sample_index\", sample_index)\n",
    "#         print(\"len(hyp.embs)\", len(hyp.embs))\n",
    "#         print(\"len(extend_embs)\", len(extend_embs) )\n",
    "#         print(\"trs\", trs)\n",
    "        stim = sm.make_variants(sample_index, hyp.embs, extend_embs, trs)\n",
    "        \n",
    "        likelihoods = em.prs(stim, trs)\n",
    "#        print(\"liklihoods: \", likelihoods)\n",
    "        local_extensions = [Hypothesis(parent = hyp, extension = x) for x in zip(nuc, logprobs, extend_embs)]\n",
    "        decoder.add_extensions(local_extensions, likelihoods, nextensions)\n",
    "    decoder.extend(verbose = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "86049506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we need to get your picture taken for id'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(decoder.beam[0].words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231104c6",
   "metadata": {},
   "outputs": [],
   "source": [
    " I'm from a very small African country called Zimbabwe. It's a country that's been in the news very recently for very many bad reasons. The one thing you might not know about Zimbabwe is that it's one of the youngest democracies in the world. Well, democracy is perhaps not the right word, so let me say it's one of the youngest countries in the world. It's only 29 years old. And the big year of change in my country, the big year of change in my family, and the big year of change for me was 1980, when my country finally became independent. Those of you, and I'm sure there are many of you here who are experts in African history, will know that in the late 50s and the early 60s, Britain, France, and the other colonial powers were giving up their colonies for a number of reasons. So countries like Nigeria became independent, Ghana became independent, but not Rhodesia. The white minority government in Rhodesia, led by somebody called Ian Smith, had other ideas for the kind of country they wanted to live in. And one of the very firm ideas they had was that they didn't want to live in a country ruled by black people. So they declared a unilateral declaration of independence from Britain, the colonial power, and the result of this was that the country was isolated from the outside. And on the inside, there was a civil war between the white minority on the one side and the black majority led by black freedom fighters. And after about 14 years of war and negotiations in England, we finally became an independent republic called Zimbabwe. I have very vivid memories of that time. We lived in the township, which were these African areas. Rhodesia was segregated along the same lines as South Africa, but on a smaller scale. So we lived in the township, where I remember around the time of independence, there was so much music. Everybody was singing, everybody was dancing, it was almost like you could actually touch the joy in the air. And the song that everybody was singing, if you'll allow me to sing it, is a song by Bob Marley called Zimbabwe. Do you know it? Then join in. Africa shall liberate Zimbabwe, Africa shall liberate Zimbabwe. It was a song that was on everybody's lips. Then the biggest thing that happened that year for a lot of people in Harare was that Bob Marley himself came to Salisbury to give a concert, an independence concert. He came all the way from Kingston with his whalers. But that was not the biggest thing that happened to me, because the biggest thing that happened to me was that the white areas, the formerly white areas, the suburbs, began to open up. And my father finally achieved the dream of a lifetime. He moved us out of the township into the suburbs to live with the white people, the good area. So imagine what it meant for a family from the township, where the only road that was tarred was the road to town. And all the other little roads were dusty, full of mud, full of dust. There was no electricity at night in some parts of the township. So imagine us in this new environment where the roads are lined with beautiful trees. In the morning, the milkman deposits two bottles of milk outside your door, one silver, one gold, depending on the amount of cream you want in your milk. The breadman rings the bell in the morning to tell you that your fresh Lobos bread is ready for you outside. And the newspaper boy throws his newspaper over the wall for you to read in the morning. I went to a school called Alfred Bight, where I found myself as one of 24 children in the classroom. Twenty of them were white. Now, I had been at a school in the township where we had something called hot seating, which meant that 48 of us came to school in the morning, and then went home to make room for another 48 people who came in the afternoon. So this was absolute paradise to me, a class of only 24 children. But the very first thing I did in my new classroom was the wrong thing. My teacher, Miss Callan, called me to her desk, and as a well-trained little African girl who had been brought up well by her teachers, I knelt before her. What are you, a goat? she said. I still remember the surprised laughter of the whole class, and it was a sound that I became very familiar with as the year went on, because it seemed everything about me was wrong, everything I did was wrong. My hair, for instance, not this hair. Well, this is my hair in the sense that I paid for it. But my hair was too curly, it was too close to my head. My English, when it came, was too slow, and the accent was very strange. And then there was the small matter of the sandwiches. You see, my mother made us egg sandwiches every morning for school. So she fried eggs, put them between butter slices of bread. That was the wrong food to take to school, because what the white children ate was something called polony, which really stank. And then they had something else called marmite, which is a yeast extract, and it's the foulest tasting substance known to man. And I just, I really wanted this stinky, horrible food, because I thought that, you know, if I ate the same food that the other people ate, if I had the same kind of hair, then I would fit in somehow. But of course, that didn't happen. So every day I had Russell Webb laughing at my hair, I had Carrie Trelaw laughing at my hair, I had Natasha Russell refusing to share her Smarties with me that she bought on holiday in South Africa. The only time that I really felt I belonged to Alfred Bight, my new school, was in the mornings at assembly. We would sit cross-legged on the floor, and Miss Roberts, our headmistress, would play the piano and lead us in the school song. It was a song about valor, about duty, about honor. It was a song about commitment and dedication. And I sang it at the top of my voice because it was the one moment when I knew peace at my school, when there was no laughter, when there was no mockery on the playground. It was a song about the pioneer column who colonized my country and turned it into Rhodesia. So there I was, a 10-year-old African child in a newly independent country called Zimbabwe, singing this song in which God regarded the conquest of my country as an act of honor. This was a song celebrating the conquest of a kingdom in Zimbabwe. It was a song celebrating the fact that many thousands of people had been made landless. The song was called, Thou Who Didst Guide Our Father's Feet. And the last sentence was, As thou hast done, do once again. I loved that song. As you can imagine, we didn't sing it for very long. My friend, Jessie Majome, who has gone on to greater things and is now actually the Deputy Minister of Justice in my country, told her father about the song that we were made to sing every morning. And her father called the Herald, which was the state newspaper. And I remember the Herald journalist coming to the school and Miss Roberts rushing across the quadrangle. And she was quite baffled by the whole thing. I'm not a racialist, she insisted. I have black children in my school. What do you mean I'm a racist? That's not it at all. It's just a tradition. It's like the school motto. It's like the recorder lessons. It's just a tradition. It doesn't mean anything. I think that this was the moment that the teachers at my school and Miss Roberts finally had to confront what it meant to live in an independent African country. It wasn't just about changing the name of the country from Rhodesia to Zimbabwe, changing the name of the capital from Sorse to Harare. It meant that not only did we have to start relating to each other differently across the racial divide, but we also had to start reevaluating our history. And for some of the teachers, I think that was a step too far. Because about a year later came the great white flight, when a lot of teachers left the school and a lot of children left the school as well. I'm not really sure that I can relate the white flight to this particular incident. But what I know is that after about a year at my new school, it finally began to resemble the kind of school you'd expect to find in an independent African country. So I finally found myself being part of a school that had all the amenities that my old township lacked, but that truly looked like a Zimbabwean school. And this is how independence came to me. Thank you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c03ceb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
